{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "264px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": "5",
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Copia de SequencePrediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quintanaMathe/causal-analysis-netsi/blob/master/SequencePrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crOQ7QBjhrgV"
      },
      "source": [
        "# Sequence Prediction: Surname Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7mWak9NZqsE"
      },
      "source": [
        "In this experiment, you will explore a simple sequence prediction task: Text generation using a Gated Recurrent Unit (GRU). Like an LSTM, this is an example of a recurrent neural network, but since it has fewer parameters, it should be more appropriate for a smaller dataset.\n",
        "\n",
        "The GRU computes a probability distribution over the set of possible characters in the surname vocabulary for each time step. We use these probability distributions to generate new surnames. You are given starter code that trains a SurnameGenerationModel on the [surname dataset](https://github.com/jasoriya/CS6120-PS2-support/blob/master/data/surnames/surnames_with_splits.csv) to generate new surnames by learning from the training data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJkLtjvSaf5P"
      },
      "source": [
        "Your task is to understand this code and plot the over all perplxity of GRU model as a function of the hidden representation size (K) and the number of characters already observed. You will see **TODO** prompts in the following cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEjYmZ5Shrgb"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIbXsKQLhrgd"
      },
      "source": [
        "import os\n",
        "from argparse import Namespace\n",
        "\n",
        "import numpy as np\n",
        "import httpimport\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "# import data preprocessing and modeling functions from https://github.com/jasoriya/CS6120-PS2-support/tree/master/utils\n",
        "with httpimport.remote_repo(['data_vectorization','model','helper'], 'https://raw.githubusercontent.com/jasoriya/CS6120-PS2-support/master/utils/'):\n",
        "  from data_vectorization import Vocabulary, SequenceVocabulary, SurnameVectorizer, SurnameDataset, generate_batches\n",
        "  from model import SurnameGenerationModel, sample_from_model, decode_samples\n",
        "  from helper import make_train_state, update_train_state, normalize_sizes, compute_accuracy, sequence_loss, set_seed_everywhere, handle_dirs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwQ3H6hhhrhM"
      },
      "source": [
        "### Settings and some prep work\n",
        "\n",
        "**TODO**: \n",
        "- Give path to a directory where the model should be saved\n",
        "- Give hidden state size (`rnn_hidden_size`) for the GRU model (experiment with different levels)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6ELbWoHhrhN"
      },
      "source": [
        "args = Namespace(\n",
        "    # Data and Path information\n",
        "    surname_csv=\"https://raw.githubusercontent.com/jasoriya/CS6120-PS2-support/master/data/surnames/surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir= \"\", # give path here\n",
        "    # Model hyper parameters\n",
        "    char_embedding_size=32,\n",
        "    rnn_hidden_size= 16, # give hidden size\n",
        "    # Training hyper parameters\n",
        "    seed=1337,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=128,\n",
        "    num_epochs=100,\n",
        "    early_stopping_criteria=5,\n",
        "    # Runtime options\n",
        "    catch_keyboard_interrupt=True,\n",
        "    cuda=True,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        "    reload_from_files=False,\n",
        ")\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "    \n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "    \n",
        "    \n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "    \n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK7rHgKPhrhS"
      },
      "source": [
        "### Initializations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zdUk04AhrhT"
      },
      "source": [
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "model = SurnameGenerationModel(char_embedding_size=args.char_embedding_size,\n",
        "                               char_vocab_size=len(vectorizer.char_vocab),\n",
        "                               rnn_hidden_size=args.rnn_hidden_size,\n",
        "                               padding_idx=vectorizer.char_vocab.mask_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy5dwODVhrhX"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "jIr6FBQWhrhY"
      },
      "source": [
        "mask_index = vectorizer.char_vocab.mask_index\n",
        "\n",
        "model = model.to(args.device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                           mode='min', factor=0.5,\n",
        "                                           patience=1)\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "epoch_bar = tqdm_notebook(desc='training routine', \n",
        "                          total=args.num_epochs,\n",
        "                          position=0)\n",
        "\n",
        "dataset.set_split('train')\n",
        "train_bar = tqdm_notebook(desc='split=train',\n",
        "                          total=dataset.get_num_batches(args.batch_size), \n",
        "                          position=1, \n",
        "                          leave=True)\n",
        "dataset.set_split('val')\n",
        "val_bar = tqdm_notebook(desc='split=val',\n",
        "                        total=dataset.get_num_batches(args.batch_size), \n",
        "                        position=1, \n",
        "                        leave=True)\n",
        "\n",
        "try:\n",
        "    for epoch_index in range(args.num_epochs):\n",
        "        train_state['epoch_index'] = epoch_index\n",
        "\n",
        "        # Iterate over training dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "        dataset.set_split('train')\n",
        "        batch_generator = generate_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        model.train()\n",
        "        \n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # the training routine is these 5 steps:\n",
        "\n",
        "            # --------------------------------------    \n",
        "            # step 1. zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # step 2. compute the output\n",
        "            y_pred = model(x_in=batch_dict['x_data'])\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
        "\n",
        "\n",
        "            # step 4. use loss to produce gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # step 5. use optimizer to take gradient step\n",
        "            optimizer.step()\n",
        "            # -----------------------------------------\n",
        "            # compute the  running loss and running accuracy\n",
        "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            # update bar\n",
        "            train_bar.set_postfix(loss=running_loss,\n",
        "                                  acc=running_acc,\n",
        "                                  epoch=epoch_index)\n",
        "            train_bar.update()\n",
        "\n",
        "        train_state['train_loss'].append(running_loss)\n",
        "        train_state['train_acc'].append(running_acc)\n",
        "\n",
        "        # Iterate over val dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "        dataset.set_split('val')\n",
        "        batch_generator = generate_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.\n",
        "        running_acc = 0.\n",
        "        model.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            y_pred = model(x_in=batch_dict['x_data'])\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
        "\n",
        "            # compute the  running loss and running accuracy\n",
        "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "            \n",
        "            # Update bar\n",
        "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
        "                            epoch=epoch_index)\n",
        "            val_bar.update()\n",
        "\n",
        "        train_state['val_loss'].append(running_loss)\n",
        "        train_state['val_acc'].append(running_acc)\n",
        "\n",
        "        train_state = update_train_state(args=args, model=model, \n",
        "                                         train_state=train_state)\n",
        "\n",
        "        scheduler.step(train_state['val_loss'][-1])\n",
        "\n",
        "        if train_state['stop_early']:\n",
        "            break\n",
        "        \n",
        "        # move model to cpu for sampling\n",
        "        model = model.cpu()\n",
        "        sampled_surnames = decode_samples(\n",
        "            sample_from_model(model, vectorizer, num_samples=2), \n",
        "            vectorizer)\n",
        "        epoch_bar.set_postfix(sample1=sampled_surnames[0], \n",
        "                              sample2=sampled_surnames[1])\n",
        "        # move model back to whichever device it should be on\n",
        "        model = model.to(args.device)\n",
        "        \n",
        "        train_bar.n = 0\n",
        "        val_bar.n = 0\n",
        "        epoch_bar.update()\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    print(\"Exiting loop\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1pV10L3hrhc"
      },
      "source": [
        "np.random.choice(np.arange(len(vectorizer.nationality_vocab)), replace=True, size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crgnIUPghrhg"
      },
      "source": [
        "# compute the loss & accuracy on the test set using the best available model\n",
        "\n",
        "model.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "model = model.to(args.device)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                   batch_size=args.batch_size, \n",
        "                                   device=args.device)\n",
        "running_acc = 0.\n",
        "model.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred = model(x_in=batch_dict['x_data'])\n",
        "\n",
        "    # compute the loss\n",
        "    loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
        "\n",
        "    # compute the accuracy\n",
        "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "train_state['test_loss'] = running_loss \n",
        "train_state['test_acc'] = running_acc "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldfJK8jBgbnT"
      },
      "source": [
        "**TODO**: Write code to compute the perplexity of the test corpus, the accuracy of a character-prediction task, and the perplexity of the predictive distribution given increasing amounts of context. For this last metric, you want to compute the perplexity of predicting the first character (given nothing to the left), the perplexity of predicting the second character (given the first), and so on, up to the perplexity of predicting the n-th character (given the first 1 to n-1 characters). Unlike an n-gram model, the recurrent model can encode arbitrarily long histories, although it will probably make more use of the nearby context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfzssGYjhrhj"
      },
      "source": [
        "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
        "print(\"Test perplexity: {};\".format(1000000)) # compute and print perplexity here\n",
        "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FCywMZ6hrhm"
      },
      "source": [
        "## Inference\n",
        "To see the names that the model generates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTbrH4dBhrho"
      },
      "source": [
        "# number of names to generate\n",
        "num_names = 10\n",
        "model = model.cpu()\n",
        "# Generate nationality hidden state\n",
        "sampled_surnames = decode_samples(\n",
        "    sample_from_model(model, vectorizer, num_samples=num_names), \n",
        "    vectorizer)\n",
        "# Show results\n",
        "print (\"-\"*15)\n",
        "for i in range(num_names):\n",
        "    print (sampled_surnames[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjgqHfp7ggAj"
      },
      "source": [
        "**TODO**: Train the GRU model given above multiple times for different levels of `rnn_hidden_size`. For each of these models, plot the average perplexity as a function of the number of characters of the name observed so far. Explain your observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsKEX4ylUUFP"
      },
      "source": [
        "# Your plotting code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}